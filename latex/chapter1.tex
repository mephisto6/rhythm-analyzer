%----------------------------------------------------------------------------
\chapter{Adatbányászati háttér}\label{sect:Adatb}
%----------------------------------------------------------------------------
\section{Az alkalmazott adatbányászatról}
%----------------------------------------------------------------------------
Az adatbányászatot sokféleképp lehet definiálni. A legelterjedtebb megfogalmazás szerint olyan újszerû, érvényes és korábban ismeretlen tudás kinyerése nagyméretû adathalmazból, mely nem triviális, hasznos és valamilyen módon magyarázható is \cite{BodonPhD}. Mint formula, nem a legszerencsésebb kifejezés, hiszen míg a szénbányászaton a szén kitermelését értjük, az adatbányászat nem új adatot, hanem a nyers adatból tudást hoz létre.

Az alkalmazott adatbányászat kétségtelenül multidiszciplináris terület, hiszen az alapjait adó tágabb értelemben vett matematikai ágak (statisztika, valószínûség-számítás, lineáris algebra, algoritmus-elmélet, mesterséges intelligencia, stb\dots) mellett nem hagyható figyelmen kívül az adathalmaz által reprezentált entitásokhoz kapcsolódó meglévõ tudásunk sem, hiszen biológusok, csillagászok, bankárok nap mint nap bár, de különbözõképp használják és magyarázzák eredményeit. A dolgozatom egy etnomuzikológus által felvetett kérdésekre keres adatbányászaton alapuló, de a elsõsorban népzenekutató szakember által értelmezhetõ válaszokat, tehát a cél, hogy a népzenekutatás számára állítsunk elõ új tudást.

%----------------------------------------------------------------------------
\section{A tudásfeltárás folyamata}
%----------------------------------------------------------------------------

Az adatbányászati folyamat eredményességének elengedhetetlen feltétele, hogy kövessük a területtel foglalkozó korábbi kutatások által meghatározott lépéseket. A témával foglalkozó szakirodalom \cite{BodonPhD, BodonBuza} a követketkezõ fázisokat sorolja fel:

\begin{enumerate}
	\item Az alkalmazási terület feltárása és megértése, a fontosabb elõzetes ismeretek begyûjtése, a felhasználási célok meghatározása
	\item Céladatbázis létrehozása: kiválasztani az adatbázist, amelybõl a tudást ki akarjuk nyerni
	\item Adatok elõfeldolgozása: téves bejegyzések eltávolítása, zajszûrés (adattisztítás)
	\item Adattér csökkentés: az adatbázisból a cél szempontjából fontos attribútumok kiemelése
	\item Az adatbányászati algoritmus típusának kiválasztása: eldönteni, hogy milyen módszert igényel a megoldandó feladat
	\item A megfelelõ algoritmus kiválasztása (idõ- és tárigény, elõnyök, hátrányok alapján)
	\item Az algoritmus futtatása
	\item A kinyert tudás értelmezése, esetleges visszalépés finomítás céljából
	\item A megszerzett tudás megerõsítése: összevetés az elvárásokkal, elõzetes ismeretekkel
\end{enumerate}

A munka során mindvégig szem elõtt tartottam, hogy amennyire csak lehetséges, az itt felsorolt pontok mindegyikét kövessem. A következõ fejezetek struktúrálása ezen ütemterv nyomán készült.

%----------------------------------------------------------------------------
\section{Klaszterezés}
%----------------------------------------------------------------------------

A klaszterezés egy olyan dimenziócsökkentõ eljárás, melynek során az elemeket homogén csoportokba, úgy nevezett klaszterekbe soroljuk \cite{Rokach}. Az egyes klasztereken belüli elemek valamilyen szempontból hasonlítanak egymáshoz és különböznek a többi klaszter elemeitõl. A csoportosítás alapját különbözõ távolságmértékek képezik. Formálisan, a klaszterezési struktúra az elemeket tartalmazó S univerzum olyan részhalmazainak halmaza ($C=C_1,..,C_k$), hogy $S=\bigcup_{i=1}^k C_i$ és $C_i \cap C_j = \emptyset $,$ \forall i \neq j$-re.

A klaszter-analízis két fõ ága a hierarchikus és a nem hierarchikus klaszterezés. Hierarchikus esetben az új klasztereket a korábbi klaszterek alapján határozzuk meg, ezzel szemben a nem hierarchikus módszerek egyszerre határozzák meg az összes klasztert.

%----------------------------------------------------------------------------
\subsection{A klaszterek és a klaszterezés jellemzõi}
%----------------------------------------------------------------------------

Egy klaszterezési algoritmus futása közben, illetve az eredmények összehasonlítása során alapvetõ kérdés, hogy hogyan értékelünk egy adott klaszterezést. Hogy ezt megtehessük, definiálnunk kell néhány klaszter-, illetve klaszterezésjellemzõt. 

A C klaszter elemeinek számát $|C|$ jelöli. A klaszter nagyságának jellemzésére szolgál az átmérõ fogalma ($D(C)$). Ezt kétféleképp is definiálhatjuk:
\begin{itemize}
\item A klaszte elemei közötti maximális távolság alapján: 
	\begin{align}
	D_{max}(C) = \max_{p,q \in C} d(p,q) 
	\label{eq:1}
	\end{align}
\item A klaszter elemei közötti átlagos távolság alapján: 
	\begin{align}
	D_{avg}(C) = \frac{\sum_{p,q \in C}d(p,q)}{|C|^2} 
	\label{eq:2}
	\end{align}
\end{itemize}

Ha a klaszterezendõ elemek olyanok, hogy vektortérben értelmezhetõek, akkor beszélhetünk a klaszter középpontjáról ($\vec m_C$), illetve sugaráról ($R_C$), mint klaszterjellemzõkrõl. Ezeket így definiáljuk:
	\begin{align}
	\vec m_C = \frac{\sum_{p \in C} \vec p}{|C|} 
	\label{eq:3}
	\end{align}
	
	\begin{align}
		R_C = \frac{\sum_{p \in C} |\vec p- \vec m_C|}{|C|} 
		\label{eq:4}
  \end{align}
	
Gyakran elõfordul, hogy egy algoritmus következõ állapotának meghatározásához meg kell találnunk egy adott célfüggvényt minimalizáló klaszterpárt. Ilyen célfüggvény lehet a klaszterek közötti aktuális távolság, melyet többféleképpen is meghatározhatunk:
\begin{itemize}
\item A két klaszter elemeinek páronként számított távolságainak minimuma:
	\begin{align}
	d_{min}(C_i,C_j) = \min_{p \in C_i,q \in C_j} d(p,q) 
	\label{eq:5}
	\end{align}
\item A két klaszter elemeinek páronként számított távolságainak maximuma:
	\begin{align}
	d_{max}(C_i,C_j) = \max_{p \in C_i,q \in C_j} d(p,q) 
	\label{eq:6}
	\end{align}
\item A két klaszter elemeinek páronként számított távolságainak átlaga:
	\begin{align}
	d_{avg}(C_i,C_j) = \frac{\sum_{ p \in C_i, q \in C_j} d(p,q)}{|C_i|*|C_j|}
	\label{eq:7}
	\end{align}
\item A két klaszterközéppont távolsága:
	\begin{align}
	d_{mean}(C_i,C_j) = |\vec m_i - \vec m_j|
	\label{eq:8}
	\end{align}
\item Az egyesítésükkel keletkezõ klaszter átmérõje
	\begin{align}
	d_D(C_i,C_j) = D(C_i \cup C_j) 
	\label{eq:9}
	\end{align}
\end{itemize}

Amennyiben már van egy klaszterezésünk, és azt össze szeretnénk hasonlítani egy másikkal, akkor egy adott klaszterezési struktúrára is definiálnunk kell mérõszámokat. A következõ függvények minimalizálása a legelterjedtebb klaszterezõ algoritmusok esetén:
\begin{itemize}
\item Hibaösszeg (klasztereken belüli elemek klaszterközéppontól való távolságainak összege):
	\begin{align}
	\sum_{i=1}^{k}\sum_{p \in C_i}| \vec p- \vec m_{C_i} |
	\label{eq:10}
	\end{align}
\item Négyzetes hibaösszeg (ugyanaz, mint az elõzõ, csak négyzetesen - a nagy távolságok súlyának növelése céljából):
	\begin{align}
	\sum_{i=1}^{k}\sum_{p \in C_i}| \vec p- \vec m_{C_i} |^2 
	\label{eq:11}
	\end{align}
\item Klasztereken belüli távolságösszegek:
	\begin{align}
	\sum_{i=1}^{k}\sum_{p,q \in C_i} d(p,q) 
	\label{eq:12}
	\end{align}
\item Maximális átmérõ:
	\begin{align}
	\max_{i \in \left\{1,\dots,k\right\}} D(C_i) 
	\label{eq:13}
	\end{align}
\item Átlagos átmérõ:
	\begin{align}
	\frac{\sum_{i \in \left\{1,\dots,k\right\}} D(C_i)}{k} 
	\label{eq:14}
	\end{align}
\end{itemize}

A következõ fejezetek algoritmusainak ismertetéséhez feltétlenül szükséges a fenti metrikák pontos definiálása, hiszen az eljárások szinte minden lépésük során használják valamelyiküket.

%----------------------------------------------------------------------------
\subsection{Hierarchikus klaszterezési módszerek}
%----------------------------------------------------------------------------

Ezek az eljárások közösek abban, hogy iteratívan, fentrõl lefelé vagy lentrõl felfelé építkezve a klaszterezés korábbi állapota alapján határozzák meg annak következõ felosztását \cite{Jain}. Az építkezés stratégiája alapján a következõképpen lehet õket alosztályokra bontani:

\begin{itemize}
	\item Agglomeráló (összegyûjtõ) hierarchikus klaszterezés: Kezdetben minden objektum egy önálló klasztert alkot, majd ezek összefésülésével jutunk el a kívánt célhoz.
	\item Divizionáló (megosztó) hierarchikus klaszterezés: Kezdetben minden objektum egy nagy közös klaszterba tartozik, majd ennek rekurzív felosztásával kapjuk meg a végsõ állapotot.
\end{itemize}

Az összefésülés illetve a felosztás valamilyen távolságmérték alapján történik. Minden lépésben egy elõre megadott jósági kritériumot optimalizálunk. A távolságértékek alapján történõ következõ lépés meghatározása alapján további felosztásokat tehetünk a hierarchikus klaszterezõ eljárásokra:

\begin{itemize}
	\item Single-linkage klaszterezés
	
	Ebben az esetben két klaszter távolságát a bennük lévõ elemek páronkénti távolságainak minimumaként definiáljuk (\ref{eq:5}-ös képlet).
	
	Gráfelméleti szempontból nézve (egy teljes gráfban a pontok a klaszterezendõ elemek, az élsúlyok pedig a pontokhoz tartozó elemek közötti távolságok) ez a módszer egy minimális feszítõfát fog találni, ha a klaszterek számát $1$-re állítjuk. Ha $k$ darab csoportot szeretnénk kapni, akkor ezt a minimális feszítõfa $k-1$ darab legnagyobb súlyú élének elhagyásával kaphatjuk meg. Az így keletkezett komponensekben található elemek kerülnek egy klaszterbe.
		
		A módszernek komoly hátránya, az úgy nevezett lánceffektus: néhány hídként viselkedõ egymáshoz közeli pont összekapcsolhat egyébként jól elkülönülõ klasztereket. További megfontolandókat vetnek fel az elkülönülõ pontok (úgynevezett outlierek) jelenléte az adatbázisban. 
		
	\item Complete-linkage klaszterezés
	
	Ez a megközelítés is a klasztereken belüli elemek páronkénti távolságából indul ki, de minimum helyett azok maximumát használja a klasztertávolságok meghatározására (\ref{eq:6}-os képlet). 
	
	Gráfelméleti szempontból itt most maximális teljes részgráfok, azaz részgráfként elõálló klikkek keresése a cél, melyek meghatároznak egy klasztert. A feltétel az, hogy minden szomszéd egy számított maximális távolságon belül legyen.

	A kapott klaszterek kompaktsága miatt gyakran ez adja a legjobb eredményeket, hiszen kiküszöböli a single linkage klaszterezésnél elõforduló lánceffektust. Jellemzõ rá, hogy a klaszterek átmérõi megközelítõleg hasonló méretûek.
	
	\item Average-linkage klaszterezés
	
	Ez a módszer két klaszter távolságát elemeik páronkénti távolságainak átlagaként definiálja (\ref{eq:7}-es képlet).
	Hátránya, hogy ,,hosszúkás'' klasztereket szétbonthat, illetve szomszédos ,,hosszúkás'' klasztereket összevonhat.
\end{itemize}

Minden további felosztás egy nagyobb távolságot eredményez a klaszterek között, mint az elõzõ felosztás maximális távolsága. A leállási feltétel így lehet egy maximális új távolság is a $k$ számú klaszter elérése mellett.
 
A hierarchikus módszerek közös hátránya a rossz skálázhatóság és a nemlineáris lépésszám (legjobb esetben is ($O(n^2)$, de akár $O(2^n)$ is lehet, ahol $n$ az elemek száma), melynek következménye lehet nagyméretû adathalmaz esetén a túl sok fájlmûvelet. Súlyos hátrány még a visszalépési képesség hiánya.

%----------------------------------------------------------------------------
\subsection{Partícionáló módszerek}
%----------------------------------------------------------------------------

Ezek a módszerek abban közösek, hogy egy kezdeti $k$ diszjunkt partícionálás után az elemeket ide-oda mozgathatják a klaszterek között, minden lépés során ügyelve arra, hogy egy elõre megadott metrikát (ilyen lehet az \ref{eq:10}, \ref{eq:11}, \ref{eq:12}, \ref{eq:13} és \ref{eq:14} függvények bármelyike) minimalizálják vagy maximalizálják. Amennyiben több lehetõség is kínálkozik a célfüggvény javítására, a legjobbat választják ezek  közül. Akkor állnak le, ha már nem lehet több olyan lépést tenni, ami javítaná az aktuális klaszterezés jóságát.

A $k$-means az egyik legrégebbi és legegyszerûbb klaszterezõ algoritmus, mely vektortérben elhelyezkedõ elemek esetén használható. Menete a következõ: kezdetben választunk $k$ darab véletlen elemet, melyek reprezentálják a $k$ darab klasztert. Ezután minden elemet hozzárendeljük ahhoz a klaszterhez, amelynek reprezentáns pontjától való tavolsága minimális. A besorolás után új reprezentatív pontot választunk, a klaszter középpontját. Innentõl kezdve pedig addig folytatjuk a besorolást és az új középpont választást, amíg történik változás.

Mivel véletlen elemekbõl indultunk ki, az algoritmust még néhányszor le kell futtatnunk és azt az eredményt kell választanunk, amelyik a célfüggvényt legjobban optimalizálja. A lépésszám így lineáris: $O(nkt)$, ahol $n$ az elemek száma, $k$ a keresett klaszterek száma, $t$ pedig az iterációk száma.

Finomítása az algoritmusnak a $k$-medoid nevû módszer, melyben a klasztert nem a középpontja reprezentálja, hanem az adatbázisban ténylegesen elõforduló, leginkább középen elhelyezkedõ (a klaszter többi pontjától minimális átlagos távolságú) elem, a medoid (formálisan egy C kalszter medoidja egy olyan $i \in C$ elem, melyre $\sum_{j \in C} d(i,j)$ minimális). Ennek elõnye, hogy egyrészt kevésbé érzékeny a kívülálló pontokra, másrészt csak a távolságértékeket használja, tehát nincs semmilyen megkötés az elemekre (nem kell vektortérben lenniük). A javulás ára költség növekedése: míg a középpont meghatározása megvan lineáris idõben, a medoid megtalálása ennél költségesebb.

A gyengepontja ezeknek az algoritmusoknak, hogy a kezdeti klaszterezés milyensége nagyban befolyásolja az eredményét. Az érzékenység abból fakad, hogy egy szerencsétlen kezdeti klaszterezés globális minimum helyett lokális minimumhoz vezethet. Ennek az esélyének csökkentésére a következõket tehetjük:

\begin{itemize}
\item Véletlenszerû választás helyett kiválaszthatjuk a két legtávolabbi elemet, majd a következõ elemeket sorra úgy, hogy a már megválasztott középpontoktó való távolságuk legyen maximális.
\item Csak az elsõ középpontot választjuk véletlenszerûen, utána az elõbbihez a hasonlóan a többit úgy választjuk, hogy a már kiválasztottaktól való távolságuk legyen a lehetõ legnagyobb.
\end{itemize}

%----------------------------------------------------------------------------
\subsection{Sûrûség-alapú módszerek}
%----------------------------------------------------------------------------

Az eddig felsorolt klaszterezõ algoritmusok két dologban voltak közösek: nulladik lépésként szükség volt a klaszterek számának ($k$) megadására, illetve csak elliptikus alakú klasztereket tudtak azonosítani. Ezen hiányosságok kiküszöbölésére dolgozták ki a sûrûség-alapú klaszterezõ módszereket, melyek alapfeltevése az, hogy egy klaszteren belül jóval nagyobb az elemek sûrûsége, mint a klaszterek között.

A legelsõ és legismertebb sûrûség-alapú klaszterezõ algoritmus a DBSCAN (\textit{Density-Based Spatial Clustering of Applications with Noise}), melynek alapötlet az, hogy egy klasztert addig növesztünk, amíg hozzá tudunk venni olyan pontot, ami egy elõre magadott $\varepsilon$ távolságnál közelebb van valamelyik eleméhez. Pontosabban úgy fogalmazhatunk, hogy egy klaszteren belüli pontokra mindig igaz, hogy adott $\varepsilon$ sugarú körön belül mindig található bizonyos ($minPts$) számú elem.

Egy $p$ elem szomszédai ($N_{eps}(p)$) azok az elemek, amelyek $p$-tõl legfeljebb $\varepsilon$ távolságra vannak. Ha $q \in N_{eps}(p)$ és $|N_{eps}(p)| \geq minPts$, akkor azt mondjuk, hogy $q$ elem $p$-bõl sûrûség alapon közvetlenül elérhetõ. Ha pedig léteznek $p=p_1,p_2,\dots,p_n=q$ elemek úgy, hogy $p_{i+1}$ sûrûség alapon közvetlenül elérhetõ $p_i$-bõl, akkor $q$ elem sûrûség alapon elérhetõ $p$-bõl. A $p$ és $q$ elemek sûrûség alapon összekötöttek, ha létezik ilyen $o$ elem, amelybõl $p$ és $q$ sûrûség alapon elérhetõ.

Ezek után a klaszterek a következõ két szabály alapján kaphatjuk meg:

\begin{enumerate}
 \item Ha $p \in C$ és $q$ sûrûség alapn elérhetõ $p$-bõl, akkor $q \in C$.
 \item Ha $p,q \in C$, akkor $p$ és $q$ sûrûség alapon összekötött.
\end{enumerate}

Az algoritmus elsõ lépésében választunk egy tetszõeleges $p$ elemet és meghatározzuk a belõle sûrûség alapon elérhetõ elemeket. Amennyiben $|N_{eps}(p)|\geq mintPts$ teljesül, kaptunk egy klasztert. A feltétel nem teljesülése nem feltétlenül jelenti azt, hogy $p$ nem tartozik egy klaszterbe sem, lehetséges, hogy egy klaszter szélén helyezkedik el. Ilyenkor csak válasszunk egy új elemet. Ha már nem lehet új elemet választani, az algoritmus véget ért. Ha egy elem ezután sem tartozik egy klaszterbe sem, akkor zajnak tekinthetjük.

A módszer fõ gyengesége, hogy rendkívül érzékeny a két paraméterre ($\varepsilon, minPts$). Sõt, ha különbözõ klaszterek sûrûsége eltérõ, akkor nem biztos, hogy lehet olyan $\varepsilon$-t és $minPts$-t megadni, ami értékelhetõ eredményhez vezet.

%----------------------------------------------------------------------------
\subsection{További módszerek}
%----------------------------------------------------------------------------

Megvalósításra nem kerültek, de megemlítendõ még néhány olyan módszer, amely nem tartozik bele a fenti kategóriák egyikébe sem. 
\begin{itemize}
 \item Spektrál módszerek

Ezek azok a módszerek, amelyek a klaszterezés során az adathalmaz távolságait tartalmazó mátrix sajátértékeit, illetve sajátvektorait használják.
\item Grid-alapú módszerek

Ezek az eljárások az elemeket rácspontokba képezik le, majd a továbbiakban csak a kapott rácspontokkal dolgoznak. Fõ elõnyük a gyorsaságuk. 
\item Modell-alapú módszerek

Az adatot valamilyen matematikai modell segítségével próbálják leírni, egyben melyek karakterizálják a beletartozó elemek tulajdonságait is. Két fõ ága a döntési fa alapú megközelítés és a neurális hálókat használó eljárás.
\item Fuzzy klaszterezés

A korábbiakkal ellentétben, ahol minden elem pontosan egy osztályba tartozott, ezek az eljárások az elemek és a klaszterek között beletartozási értéket számolnak, mely annál nagyobb, minél inkább biztos, hogy az elem az adott klaszter tagja.
\item Genetikus algoritmusok

Az evolúciót utánzó genetikus algorimusok lassan mindenütt megjelennek, ezért itt is meg kell említeni õket. 
\end{itemize}
